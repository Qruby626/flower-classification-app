{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Flower Classification with MobileNetV2\n",
                "\n",
                "## Project Rules Checklist\n",
                "- **Architecture**: MobileNetV2 (Transfer Learning)\n",
                "- **Dataset**: tf_flowers\n",
                "- **Preprocessing**: 224x224, Normalized (0-1)\n",
                "- **Training**: >10 Epochs, Batch 32, Adam Optimizer\n",
                "- **Features**: Grad-CAM, Fine-Tuning, TFLite Export\n",
                "- **Split**: 80% Train (with Val), 20% Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Environment Setup & Drive Mount\n",
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import pathlib\n",
                "import shutil\n",
                "from tensorflow.keras.applications import MobileNetV2\n",
                "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
                "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
                "from tensorflow.keras.models import Model, load_model\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "import seaborn as sns\n",
                "\n",
                "# Mount Google Drive\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    print(\"Google Drive mounted successfully.\")\n",
                "    \n",
                "    # Create Project Directory in Drive\n",
                "    project_root = '/content/drive/MyDrive/flower-classification-app'\n",
                "    model_dir = os.path.join(project_root, 'model')\n",
                "    os.makedirs(model_dir, exist_ok=True)\n",
                "    print(f\"Project directory created/verified at: {project_root}\")\n",
                "    print(f\"Models will be saved to: {model_dir}\")\n",
                "    print(\"Access your files at: https://drive.google.com/drive/my-drive\")\n",
                "    \n",
                "    checkpoint_path = os.path.join(model_dir, 'flower_model_best.keras')\n",
                "    final_model_path = os.path.join(model_dir, 'flower_model_final.keras')\n",
                "    tflite_path = os.path.join(model_dir, 'flower_model.tflite')\n",
                "\n",
                "except ImportError:\n",
                "    print(\"Not running in Google Colab or Drive already mounted.\")\n",
                "    checkpoint_path = 'flower_model_best.keras'\n",
                "    final_model_path = 'flower_model_final.keras'\n",
                "    tflite_path = 'flower_model.tflite'\n",
                "\n",
                "# Install split-folders for easy data splitting\n",
                "!pip install split-folders\n",
                "import splitfolders\n",
                "\n",
                "print(f\"TensorFlow Version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Dataset Preparation (tf_flowers)\n",
                "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
                "data_file = tf.keras.utils.get_file(origin=dataset_url, fname='flower_photos', untar=True)\n",
                "print(f\"Dataset downloaded to: {data_file}\")\n",
                "\n",
                "# Robustly find the directory containing the class folders\n",
                "base_extract_dir = pathlib.Path(data_file).parent\n",
                "data_dir = None\n",
                "\n",
                "# Look for the folder that contains specific flower classes\n",
                "for root, dirs, files in os.walk(base_extract_dir):\n",
                "    if 'daisy' in dirs and 'dandelion' in dirs:\n",
                "        data_dir = pathlib.Path(root)\n",
                "        break\n",
                "\n",
                "if data_dir is None:\n",
                "    # Fallback attempt\n",
                "    possible_dir = base_extract_dir / 'flower_photos'\n",
                "    if possible_dir.exists():\n",
                "        data_dir = possible_dir\n",
                "\n",
                "if data_dir is None or not data_dir.exists():\n",
                "    raise ValueError(\"Could not verify dataset structure. Check download path.\")\n",
                "\n",
                "print(f\"Verified dataset source directory: {data_dir}\")\n",
                "print(f\"Classes found: {[d.name for d in data_dir.iterdir() if d.is_dir()]}\")\n",
                "\n",
                "# Output directory for split data\n",
                "output_dir = '/content/flower_data_split'\n",
                "\n",
                "# FORCE CLEANUP: Remove existing split folder to ensure fresh start\n",
                "if os.path.exists(output_dir):\n",
                "    shutil.rmtree(output_dir)\n",
                "    print(f\"Removed existing {output_dir}\")\n",
                "\n",
                "# Split Data: 80% Train (Validation comes from this), 20% Test\n",
                "# We will split into: Train (64%), Val (16%), Test (20%) -> Total 100%\n",
                "print(\"Splitting dataset... (this may take a moment)\")\n",
                "splitfolders.ratio(data_dir, output=output_dir, seed=1337, ratio=(.64, .16, .2), group_prefix=None)\n",
                "print(\"Split completed.\")\n",
                "\n",
                "train_dir = os.path.join(output_dir, 'train')\n",
                "val_dir = os.path.join(output_dir, 'val')\n",
                "test_dir = os.path.join(output_dir, 'test')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Data Preprocessing & Augmentation\n",
                "IMG_SIZE = (224, 224)\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "# Training Generator with Augmentation\n",
                "train_datagen = ImageDataGenerator(\n",
                "    preprocessing_function=preprocess_input, # Optimized for MobileNetV2        # Normalize 0-1\n",
                "    rotation_range=20,\n",
                "    width_shift_range=0.2,\n",
                "    height_shift_range=0.2,\n",
                "    shear_range=0.2,\n",
                "    zoom_range=0.2,\n",
                "    horizontal_flip=True,\n",
                "    fill_mode='nearest'\n",
                ")\n",
                "\n",
                "# Validation & Test Generators (Rescale Only)\n",
                "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
                "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
                "\n",
                "print(\"Loading Training Data...\")\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "    train_dir,\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical'\n",
                ")\n",
                "\n",
                "print(\"Loading Validation Data...\")\n",
                "val_generator = val_datagen.flow_from_directory(\n",
                "    val_dir,\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical'\n",
                ")\n",
                "\n",
                "print(\"Loading Test Data...\")\n",
                "test_generator = test_datagen.flow_from_directory(\n",
                "    test_dir,\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical',\n",
                "    shuffle=False # Important for evaluation\n",
                ")\n",
                "\n",
                "if train_generator.n == 0:\n",
                "    raise ValueError(f\"No training images found in {train_dir}. Please check the dataset path.\")\n",
                "else:\n",
                "    print(f\"Found {train_generator.n} training images.\")\n",
                "\n",
                "class_names = list(train_generator.class_indices.keys())\n",
                "print(\"Classes:\", class_names)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Model Architecture (Transfer Learning)\n",
                "Using **MobileNetV2** pre-trained on ImageNet as the base. We freeze the base layers and add a custom classification head."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
                "\n",
                "# Freeze base model\n",
                "base_model.trainable = False\n",
                "\n",
                "# Custom Head\n",
                "x = base_model.output\n",
                "x = GlobalAveragePooling2D()(x)\n",
                "x = Dense(128, activation='relu')(x)\n",
                "x = Dropout(0.5)(x)\n",
                "predictions = Dense(len(class_names), activation='softmax')(x)\n",
                "\n",
                "model = Model(inputs=base_model.input, outputs=predictions)\n",
                "\n",
                "model.compile(optimizer=Adam(learning_rate=0.001),\n",
                "              loss='categorical_crossentropy',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Training (Stage 1)\n",
                "\n",
                "# Use the path defined in Step 1\n",
                "print(f\"Saving best model to: {checkpoint_path}\")\n",
                "\n",
                "# Callbacks\n",
                "checkpoint = ModelCheckpoint(checkpoint_path, \n",
                "                             monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
                "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
                "\n",
                "print(\"Starting Training...\")\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    epochs=15,\n",
                "    validation_data=val_generator,\n",
                "    callbacks=[checkpoint, early_stop]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "def plot_history(history, title=\"Model Performance\"):\n",
                "    acc = history.history['accuracy']\n",
                "    val_acc = history.history['val_accuracy']\n",
                "    loss = history.history['loss']\n",
                "    val_loss = history.history['val_loss']\n",
                "    epochs_range = range(len(acc))\n",
                "\n",
                "    plt.figure(figsize=(12, 4))\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
                "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
                "    plt.legend(loc='lower right')\n",
                "    plt.title('Training and Validation Accuracy')\n",
                "\n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.plot(epochs_range, loss, label='Training Loss')\n",
                "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
                "    plt.legend(loc='upper right')\n",
                "    plt.title('Training and Validation Loss')\n",
                "    plt.suptitle(title)\n",
                "    plt.show()\n",
                "\n",
                "plot_history(history, \"Initial Training\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 6. Fine-Tuning (Stage 2)\n",
                "Unfreezing the top layers of the base model to adapt them to the flower features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_model.trainable = True\n",
                "\n",
                "# Fine-tune from this layer onwards\n",
                "fine_tune_at = 100\n",
                "\n",
                "# Freeze all the layers before the `fine_tune_at` layer\n",
                "for layer in base_model.layers[:fine_tune_at]:\n",
                "  layer.trainable = False\n",
                "\n",
                "# Compile with Lower Learning Rate\n",
                "model.compile(optimizer=Adam(learning_rate=1e-5),  # Low LR for fine-tuning\n",
                "              loss='categorical_crossentropy',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "fine_tune_epochs = 10\n",
                "total_epochs =  len(history.history['accuracy']) + fine_tune_epochs\n",
                "\n",
                "history_fine = model.fit(\n",
                "    train_generator,\n",
                "    epochs=total_epochs,\n",
                "    initial_epoch=history.epoch[-1],\n",
                "    validation_data=val_generator,\n",
                "    callbacks=[checkpoint, early_stop]\n",
                ")\n",
                "\n",
                "plot_history(history_fine, \"Fine-Tuning Performance\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 7. Evaluation & Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_loss, test_acc = model.evaluate(test_generator)\n",
                "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
                "\n",
                "# Confusion Matrix\n",
                "predictions = model.predict(test_generator)\n",
                "y_pred = np.argmax(predictions, axis=1)\n",
                "y_true = test_generator.classes\n",
                "\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
                "plt.ylabel('Prediction')\n",
                "plt.xlabel('Truth')\n",
                "plt.title('Confusion Matrix')\n",
                "plt.show()\n",
                "\n",
                "print(classification_report(y_true, y_pred, target_names=class_names))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 8. Model Explainability (Grad-CAM)\n",
                "Visualizing which parts of the image the model focuses on."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "\n",
                "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
                "    grad_model = Model(\n",
                "        inputs=[model.inputs],\n",
                "        outputs=[model.get_layer(last_conv_layer_name).output, model.output]\n",
                "    )\n",
                "\n",
                "    with tf.GradientTape() as tape:\n",
                "        last_conv_layer_output, preds = grad_model(img_array)\n",
                "        if pred_index is None:\n",
                "            pred_index = tf.argmax(preds[0])\n",
                "        class_channel = preds[:, pred_index]\n",
                "\n",
                "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
                "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
                "\n",
                "    last_conv_layer_output = last_conv_layer_output[0]\n",
                "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
                "    heatmap = tf.squeeze(heatmap)\n",
                "\n",
                "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
                "    return heatmap.numpy()\n",
                "\n",
                "def display_gradcam(img_path, heatmap, alpha=0.4):\n",
                "    img = cv2.imread(img_path)\n",
                "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
                "    heatmap = np.uint8(255 * heatmap)\n",
                "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
                "\n",
                "    superimposed_img = heatmap * alpha + img\n",
                "    superimposed_img = np.clip(superimposed_img, 0, 255).astype('uint8')\n",
                "\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.imshow(img)\n",
                "    plt.title(\"Original\")\n",
                "    plt.axis('off')\n",
                "\n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.imshow(superimposed_img)\n",
                "    plt.title(\"Grad-CAM\")\n",
                "    plt.axis('off')\n",
                "    plt.show()\n",
                "\n",
                "# Get last conv layer from MobileNetV2\n",
                "last_conv_layer_name = \"out_relu\" # This is typically the last conv block in MobileNetV2\n",
                "\n",
                "# Pick a sample image from test set\n",
                "img_path = os.path.join(test_dir, class_names[0], os.listdir(os.path.join(test_dir, class_names[0]))[0])\n",
                "img = tf.keras.preprocessing.image.load_img(img_path, target_size=IMG_SIZE)\n",
                "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
                "img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
                "\n",
                "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
                "display_gradcam(img_path, heatmap)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 9. Model Export (TFLite)\n",
                "Converting the model for mobile deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Keras Model\n",
                "print(f\"Saving final model to {final_model_path}...\")\n",
                "model.save(final_model_path)\n",
                "\n",
                "# Convert to TFLite\n",
                "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "converter.optimizations = [tf.lite.Optimize.DEFAULT] # Dynamic range quantization\n",
                "tflite_model = converter.convert()\n",
                "\n",
                "with open(tflite_path, 'wb') as f:\n",
                "    f.write(tflite_model)\n",
                "\n",
                "print(f\"Model saved as {tflite_path} with size:\", len(tflite_model), \"bytes\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
